{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "RAGAs needs: \n",
    "    Ground Truth: Actual Document --> URL connection to url in doc dataset\n",
    "    Answer: RAG output\n",
    "    Contexts: passage texts in passages\n",
    "    Question: query and query id\n",
    "\n",
    "\n",
    "Merge datasests on matching URLs, with columns:\n",
    "    From msm_doc: url, title, headings, body\n",
    "    From msm_QA:url, passage_txt, query, query_id\n",
    "\n",
    "How to split the data:\n",
    "    contexts: all passage_texts based on query_id\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "# qa_path = open('MSMARCO-Question-Answering/eval_v2.1_public.json')\n",
    "qa_path = open('MSMARCO-Question-Answering/train_v2.1.json')\n",
    "# qa_path = open('MSMARCO-Question-Answering/dev_v2.1.json')\n",
    "\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data_dict = json.load(qa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict is a nested dictionary\n",
    "# here we create individial dictionaries for each column\n",
    "passages = data_dict[\"passages\"]\n",
    "query = data_dict[\"query\"]\n",
    "query_id = data_dict[\"query_id\"]\n",
    "query_type = data_dict['query_type']\n",
    "answers = data_dict[\"answers\"]\n",
    "\n",
    "#now we convert the seaparate dictionaries to a dataframe\n",
    "rows = []\n",
    "for outer_key, items in passages.items():\n",
    "    for item in items:\n",
    "        item['key'] = outer_key\n",
    "        rows.append(item)\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df['query'] = df['key'].map(query)\n",
    "df['query_id'] = df['key'].map(query_id)\n",
    "df['query_type'] = df['key'].map(query_type)\n",
    "df['answers'] = df['key'].map(answers)\n",
    "\n",
    "#format the passages to get it ready for context\n",
    "passages = []\n",
    "for passage_txt in df['passage_text']:\n",
    "    passages.append(passage_txt)\n",
    "    # passages = \" \".join([passages, str(passage)])\n",
    "# print(passages.lstrip())\n",
    "# print(passages[0:2])\n",
    "\n",
    "df['contexts'] = passages\n",
    "print(type(df['contexts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns format\n",
    "# print(len(df['key']))\n",
    "print(\"df\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Count the unique values in the 'category' column\n",
    "# unique_category_count = df['key'].nunique()\n",
    "# print(f\"Number of unique categories: {unique_category_count}\")\n",
    "\n",
    "#now we read in the MSMarco documents dataset\n",
    "docs_path = \"./datasets/TREC_RAG_2024/msmarco_v2.1_doc/msmarco_v2.1_doc_00.json.gz\"\n",
    "\n",
    "#read in the .json.gz zip\n",
    "#output is a list of strings with dictionary formating\n",
    "data = []\n",
    "gz = gzip.open(docs_path, 'rb')\n",
    "f = io.BufferedReader(gz)\n",
    "\n",
    "for line in f:\n",
    "    line = line.decode('utf-8')\n",
    "    # print(\"line\", line)\n",
    "    data.append(line)\n",
    "gz.close()\n",
    "\n",
    "# print(type(data))\n",
    "length = len(data)\n",
    "#193732 in the MSMarco_docs00\n",
    "# print(\"number of documents in data:\", length) \n",
    "# print(type(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now convert the strings to dictionaries to put into a dataframe\n",
    "# Initialize an empty list to hold the dictionaries\n",
    "dictionaries = []\n",
    "\n",
    "# Iterate over the list of strings\n",
    "for item_str in data:\n",
    "    try:\n",
    "        # Parse the string into a dictionary\n",
    "        item_dict = json.loads(item_str)\n",
    "        # Append the dictionary to the list\n",
    "        dictionaries.append(item_dict)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Print the list of dictionaries\n",
    "# for d in dictionaries:\n",
    "#     print(type(d))\n",
    "\n",
    "df_docs = pd.DataFrame(dictionaries)\n",
    "print(\"df_docs\",len(df_docs))\n",
    "\n",
    "# see what this looks like\n",
    "# print(df['passage_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now merge with QA on matching urls\n",
    "# see if you need to remove the query_ids\n",
    "# implement them into ragas\n",
    "\n",
    "MSMarco_QA_Docs = pd.merge(df, df_docs, on = \"url\", how = \"inner\")\n",
    "print(len(MSMarco_QA_Docs))\n",
    "MSMarco_QA_Docs = MSMarco_QA_Docs.rename(columns = {'query':\"question\", \"answers\":\"answer\", \"body\":\"ground_truth\"})\n",
    "\n",
    "#split the data into batches to run evaluaiton and metrics better\n",
    "df_list = list()\n",
    "for i in range(12):\n",
    "    # temp_df = df.sample(1000)\n",
    "    temp_df = MSMarco_QA_Docs.sample(1000)\n",
    "    df_list.append(temp_df) # list of 12 df's 1000 long each\n",
    "    \n",
    "print(\"df_list:\", len(df_list))\n",
    "print(\"df_list:\", df_list[0])\n",
    "\n",
    "MSMarco_QA_Docs.to_csv('MSMarco_QA_Docs_sample.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
